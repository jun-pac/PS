### Domain Adaptation in Graphs

In our study, focusing on chronological split, the environment is defined by the temporal information associated with the nodes. Specifically, the set of possible temporal information, $\bold{T}=\{\dots, t_{c}-1, t_{c}\}$, constitutes the environment set, with $t$, the temporal information, being the random variable of the environment. A dataset is considered invariant if and only if its distribution does not depend on $t$.

Despite its significance, studies on domain adaptation in GNNs are relatively scarce. This scarcity can be attributed to several factors: data corresponding to different environments may have interdependencies, real-world graph datasets often impose stringent scalability requirements, and the extrapolating nature of environments. In this section, we analyze the characteristics of domain adaptation in graphs and introduce the challenges we aim to address through our research.

Specifically, in transductive graph learning tasks, defining separate environments is challenging due to the interdependency among the data. GNN models take the entire graph as input to predict labels for all nodes, i.e., $\bold{f} : \bold{X} \rightarrow \bold{Y}$. By confining the discussion to spatial GNNs, this problem can be mitigated by defining the ego-graph $C_g(i) = \left(i, \mathcal{N}_k(i), \{(u,v) \mid u,v \in \mathcal{N}_k(i)\}\right)$, where $\mathcal{N}_k(i)$ denotes the set of neighbor nodes within a distance of $k$ from node $i$. Additionally, in practical GNNs, the process of aggregating information from the ego-graph $C_g(i)$ to classify the target node $i$ is inherently learnable. This implies that the extraction of invariant information, as depicted in the equation above, cannot be separated from the downstream function $f$. To simplify the discussion, we have confined our analysis to decoupled GNNs.

To our knowledge, there are no studies proposing invariant learning methods applicable to large-scale graphs. Notably, EERM \cite{wu2022handling} defines a graph editor that adversarially modifies the graph to obtain invariant features through reinforcement learning, which fundamentally cannot be applied to decoupled GNNs. SR-GNN \cite{shift_robust} adjusts the distance of the probability distribution of representations using a regularizer, which has a computational complexity proportional to the square of the number of nodes.

When domain shift is caused by chronological split, $e^{te}$ clearly cannot be considered a part of $\epsilon^{tr}$. Therefore, it is essential to construct $\epsilon^{all}$, which effectively includes $\epsilon^{te}$. However, we are not interested in all possible $\epsilon^{all}$. Including overly general cases within $\epsilon^{all}$ can lead to suboptimal bounds on risk. Theorems \ref{thm:env1} and \ref{thm:env2} address this issue. All proofs are provided in Appendix \ref{apdx:proofs}.