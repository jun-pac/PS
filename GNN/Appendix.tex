\subsection{Toy experiment}
\label{apdx:toy_experiment}


\subsection{Explanation of \PMP}
\label{apdx:PMP}

As one approach to achieving 1st moment alignment, we propose Persistent Message Passing (\PMP).

The reason for the dependency on the target node's time during averaging message passing is that the distribution of relative times held by neighboring nodes varies. Let $t_i$ denote the time of the target node and $t_j$ denote the time of a neighboring node. According to Assumption 3, the distribution of times held by neighboring nodes depends on the difference in absolute relative times. Specifically, for $\Delta=  | t_j - t_i|$ where $0<\Delta \le | t_{max}-t_i|$, both $t_i + \Delta$ and $t_i - \Delta$ neighbor nodes can exist. However, nodes with $\Delta >|t_{max}-t_i|$ are only possible when $t_j=t_i-\Delta$. Consequently, certain ranges of relative time receive twice the weighting in the averaging process, depending on $t_i$.

The motivation behind \PMP is to correct this by incorporating neighboring nodes that satisfy $| t_j - t_i | =0$ or $| t_j - t_i | > | t_{max}-t_i |$ with double weighting.

\begin{figure}[hbt!]
	\vspace{-0.15in}
	\centering
	\includegraphics[width=0.20\textwidth]{figs/PMP.png}
	\vspace{-0.1in}
	\caption{Grapical explanation of Persistent Message Passing(PMP).}
	 \label{fig:PMP}
	 \vspace{-0.3in}
\end{figure}

Formally, when defined as $\mathcal{N}^{\text{single}}_i=\{v\in \mathcal{N}_i \big| t_v=t_i \text{ or }  t_v < 2t_i - t_{max} \}$ and $\mathcal{N}^{\text{double}}_i=\{v\in \mathcal{N}_i\big| | t_v-t_i|\le| t_{max} - t_i | \}$, the message passing from the $k$-th layer to the $k+1$-th layer is as follows:

\begin{align}
M_{i}^{pmp(k+1)} ={{2\sum_{v\in \mathcal{N}^{\text{single}}_i} \bold{x}_{v}^{(k)}+\sum_{v\in \mathcal{N}^{\text{double}}_i} \bold{x}_{v}^{(k)}}\over{2\big|\mathcal{N}^{\text{single}}_i\big| + \big|\mathcal{N}^{\text{double}}_i\big|}}
\end{align}

Alternatively, when defined as $\mathcal{N}_i (y,t) = \{v \in \mathcal{N}_i \mid y_v=y, t_v=t \}$, $\bold{T}_{\tau}^{\text{single}}= \{t \in \bold{T}\ \big |\  t =\tau \text{ or }t<2\tau-t_{max}\}$, and $\bold{T}_{\tau}^{\text{double}}= \{t \in \bold{T}\ \big | \ |t- \tau| \le|t_{max}-\tau|, t\neq \tau\}$, the message passing mechanism of \PMP can be expressed as followes:

\begin{align}
M_{i}^{pmp(k+1)} = {\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}\sum_{v\in \mathcal{N}_{i}(y, t) }2\bold{x}_v^{(k)}+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\sum_{v\in \mathcal{N}_{i}(y, t) }\bold{x}_v^{(k)}\over \sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2|\mathcal{N}_{i}(y, t)|+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{double}}}|\mathcal{N}_{i}(y, t)|}\\={\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}{|\mathcal{N}_i(y,t)|\over |\mathcal{N}_i|}2{\sum_{v\in \mathcal{N}_{i}(y, t) }\bold{x}_v^{(k)} \over |\mathcal{N}_i(y,t)|}+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}{|\mathcal{N}_i(y,t)|\over |\mathcal{N}_i|}{\sum_{v\in \mathcal{N}_{i}(y, t) }\bold{x}_v^{(k)} \over |\mathcal{N}_i(y,t)|}\over \sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2{|\mathcal{N}_{i}(y, t)|\over| \mathcal{N}_i|}+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{double}}}{|\mathcal{N}_{i}(y, t)|\over| \mathcal{N}_i|}}\\ \simeq {\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2{\mathcal{P}_{y_i t_i}(y,t)}{\sum_{v\in \mathcal{N}_{i}(y, t) }\bold{x}_v^{(k)} \over |\mathcal{N}_i(y,t)|}+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}{\mathcal{P}_{y_i t_i}(y,t)}{\sum_{v\in \mathcal{N}_{i}(y, t) }\bold{x}_v^{(k)} \over |\mathcal{N}_i(y,t)|}\over \sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2{\mathcal{P}_{y_i t_i}(y,t)}+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{double}}}{\mathcal{P}_{y_i t_i}(y,t)}}
\end{align}

Suppose the representations from the previous layer are invariant, i.e., $\mathbb{E}_{\bold{x}\sim {X_{yt}^{(k)}}}\left[\bold{x}\right]=\mu_{X}^{(k)}(y)$. In this case, the expectation of the aggregated message is as follows.

\begin{align}
\mathbb E\left[{M_{ i}^{pmp(k+1)}}\right] = {\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t) \mu_{X}^{(k)}(y)+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t) \mu_{X}^{(k)}(y)\over \sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)}\\={\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\right)\mu_{X}^{(k)}(y)\over\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\right)}
\end{align}

By assumption 3,

\begin{align}
\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t) \\=f(y_i, t_i )\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2g(y_i, y, |t_i - t|)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}g(y_i, y, |t_i - t|)\right)
\end{align}

\begin{align}
f(y_i, t_i )\left(2g(y_i, y, 0)+2\sum_{\tau>|t_{max}-t_i |}g(y_i, y,\tau)+\sum_{0<\tau\le|t_{max}-t_i|}g(y_i, y, \tau)\right) \\= 2f(y_i, t_i )\sum_{\tau\ge 0}g(y_i, y, \tau)
\end{align}

Substituting this into the previous expression yields,

\begin{align}
\mathbb E\left[{M_{ i}^{pmp(k+1)}}\right]={\sum_{y\in \bold{Y}}\sum_{\tau\ge 0}g(y_i, y, \tau)\mu_{X}^{(k)}(y)\over\sum_{y\in \bold{Y}}\sum_{\tau\ge 0}g(y_i, y, \tau)}
\end{align}

Since there is no $t_i$ term in this expression, the mean of this aggregated message is invariant with respect to the target node's time.

\input{algorithms/PMP_message}
\input{algorithms/PMP_recon}











\subsection{Explanation of \MMP}
\label{apdx:MMP}
\PMP is not the only method for achieving 1st order alignment. There can be infinitely many ways to adjust the distribution of absolute relative times to be consistent regardless of the target node's time. Introducing Monodirectional Message Passing (\MMP) as one such approach.

\MMP aggregates information only from neighboring nodes whose times are the same as or earlier than the target node. In other words, it is a message passing function that collects and averages information only from past nodes among adjacent nodes.

\begin{figure}[hbt!]
	\vspace{-0.15in}
	\centering
	\includegraphics[width=0.20\textwidth]{figs/MMP.png}
	\vspace{-0.1in}
	\caption{Grapical explanation of Mono-directional Message Passing(MMP).}
	 \label{fig:MMP}
	 \vspace{-0.3in}
\end{figure}

\begin{align}
M_{i}^{mmp(k+1)} = {{\sum_{v\in \{v\in \mathcal{N}_i \mid t_v \le t_i \}} \bold{x}_{v}^{(k)}}\over{\big|\{v\in \mathcal{N}_i \mid t_v \le t_i \}\big| }} \simeq {\sum_{y\in \bold{Y}}\sum_{t\le t_i}{\mathcal{P}_{y_i t_i}(y,t)}{\sum_{v\in \mathcal{N}_{i}(y, t) }\bold{x}_v^{(k)} \over |\mathcal{N}_i(y,t)|}\over \sum_{y\in \bold{Y}}\sum_{t\le t_i}{\mathcal{P}_{y_i t_i}(y,t)}}
\end{align}


Applying assumption 3 as in \PMP, the expectation is as follows.

\begin{align}
\mathbb E\left[{M_{ i}^{mmp(k+1)}}\right] = {\sum_{y\in \bold{Y}}\sum_{t\le t_i }\mathcal{P}_{y_i t_i}(y, t) \mu_{X}^{(k)}(y)\over \sum_{y\in \bold{Y}}\sum_{t\le t_i }\mathcal{P}_{y_i t_i}(y, t)}={\sum_{y\in \bold{Y}}\sum_{\tau\ge 0}g(y_i, y,\tau) \mu_{X}^{(k)}(y)\over \sum_{y\in \bold{Y}}\sum_{\tau\ge 0}g(y_i, y,\tau)}
\end{align}


This also lacks the $t_i$ term, thus it is invariant.








\subsection{Estimation of relative connectivity}
\label{apdx:rel_con}

When $t_i \neq t_{max}$ and $t_j \neq t_{max}$, $\mathcal{P}_{y_i t_i} (y_j ,t_j)$ has the following best unbiased estimator:
\begin{align}
\hat{\mathcal{P}}_{y_i t_i} (y_j ,t_j)={\sum_{u\in \{v\in \bold{V} | y_v=y_i, t_v=t_i\}}|\{v\in \mathcal{N}_u | y_v=y_j, t_v=t_j\}|\over \sum _{u\in \{v\in \bold{V} | y_v=y_i, t_v=t_i\}}|\mathcal{N}_u|} , \ \forall t_i, t_j \neq t_{max}
\end{align}


When $t_i=t_{max}$ or $t_j=t_{max}$ is not feasible due to the unavailability of labels in the test set. We utilize assumption 3 to compute $\hat{\mathcal{P}}_{y_i t_i} (y_j ,t_j)$ for this cases. Let's first consider the following equation:

\begin{align}
\sum_{y_j\in\bold Y}\mathcal{P}_{y_it_i}(y_j, t_i) = \sum_{y_j \in \bold{Y}} f(y_i, t_i)g(y_i, y_j, 0) =f(y_i, t_i)\sum_{y_j \in \bold{Y}}g(y_i, y_j, 0)
\end{align}

Earlier, when introducing assumption 3, we defined $\sum_{y_j \in \bold{Y}}g(y_i, y_j, 0)=1$. Therefore, when $t_i<t_{max}$, we can express $f(y_i, t_i)$ as follows:

\begin{align}
f(y_i, t_i)=\sum_{y_j\in\bold Y}\mathcal{P}_{y_it_i}(y_j, t_i)
\end{align}

For any $\Delta \in \{|t_2 -t_1 | \mid t_1, t_2\in \bold{T}\}$, we have:

\begin{align}
\sum_{t_< t_{max}-\Delta}\mathcal{P}_{y_it_i}(y_j, t_i+\Delta) =\sum_{t_< t_{max}-\Delta}f(y_i, t_i)g(y_i, y_j, \Delta)
\end{align}

\begin{align}
\sum_{t_i<t_{max}}\mathcal{P}_{y_it_i}(y_j, t_i-\Delta) =\sum_{t_i<t_{max}}f(y_i, t_i)g(y_i, y_j, \Delta)
\end{align}

The reason we consider up to $t_i= {t_{max}-1-\Delta}$ in the first equation and up to $t_i = t_{max}-1$ in the second equation is because we assume situations where ${\mathcal{P}}_{y_i t_i} (y_j ,t_j)$ cannot be estimated when $t_i=t_{max}$ or $t_j=t_{max}$. Utilizing both equations aims to construct an estimator using as many measured values as possible when $t_i\neq t_{max}$.

Thus,

\begin{align}
g(y_i, y_j, \Delta)= {\sum_{t_i< t_{max}-\Delta}\mathcal{P}_{y_it_i}(y_j, t_i+\Delta)+\sum_{t_i<t_{max}} \mathcal{P}_{y_it_i}(y_j, t_i-\Delta)\over \sum_{t_i< t_{max}-\Delta}f(y_i, t_i)+\sum_{t_i<t_{max}}f(y_i, t_i)}
\end{align}

Since $f(y_i, t_i)=\sum_{y_j\in\bold Y}\mathcal{P}_{y_it_i}(y_j, t_i)$,

\begin{align}
g(y_i, y_j, \Delta)= {\sum_{t_< t_{max}-\Delta}\mathcal{P}_{y_it_i}(y_j, t_i+\Delta)+\sum_{t_i<t_{max}} \mathcal{P}_{y_it_i}(y_j, t_i-\Delta)\over \sum_{t_< t_{max}-\Delta}\sum_{y\in\bold Y}\mathcal{P}_{y_it_i}(y, t_i)+\sum_{t_i<t_{max}}\sum_{y\in\bold Y}\mathcal{P}_{y_it_i}(y, t_i)}
\end{align}

For any $y_1, y_2 \in \bold{Y}$ and $\Delta \in \{|t_2 -t_1 | \mid t_1, t_2\in \bold{T}\}$, we can construct an estimator $\hat{g}(y_i, y_j, \Delta)$ for $g(y_i, y_j, \Delta)$ as follows.

\begin{align}
\hat{g}(y_i, y_j, \Delta)= {\sum_{t_< t_{max}-\Delta}\hat{\mathcal{P}}_{y_it_i}(y_j, t_i+\Delta)+\sum_{t_i<t_{max}} \hat{\mathcal{P}}_{y_it_i}(y_j, t_i-\Delta)\over \sum_{t_< t_{max}-\Delta}\sum_{y\in\bold Y}\hat{\mathcal{P}}_{y_it_i}(y, t_i)+\sum_{t_i<t_{max}}\sum_{y\in\bold Y}\hat{\mathcal{P}}_{y_it_i}(y, t_i)}
\end{align}

This estimator is designed to utilize as many measured values $\hat{\mathcal{P}}_{y_i t_i} (y_j ,t_j)$ as possible, excluding cases where $t_i=t_{max}$ or $t_j=t_{max}$.

\begin{align}
\mathcal P_{y_i t_i}(y_j, t_j)= {\mathcal P_{y_i t_i}(y_j, t_j)\over \sum_{y\in \bold{Y}}\sum_{t\in\bold{T}}\mathcal{P}_{y_i t_i}(y, t)}={f(y_i,t_i)g(y_i, y_j, |t_j-t_i|)\over \sum_{y\in \bold{Y}}\sum_{t\in\bold{T}}f(y_i,t_i)g(y_i, y, |t-t_i|)}\\={g(y_i, y_j, |t_j-t_i|)\over \sum_{y\in \bold{Y}}\sum_{t\in\bold{T}}g(y_i, y, |t-t_i|)}
\end{align}

Therefore, for all $y_1, y_2 \in \bold{Y}$ and $|t_j - t_i |\in\{|t_2 -t_1 | \mid t_1, t_2\in \bold{T}\}$, we can define the estimator $\hat{\mathcal P}_{y_i t_i}(y_j, t_j)$ of $\mathcal P_{y_i t_i}(y_j, t_j)$ as follows:

\begin{align}
\hat{\mathcal P}_{y_i t_i}(y_j, t_j)={\hat{g}(y_i, y_j, |t_j-t_i|)\over \sum_{y\in \bold{Y}}\sum_{t\in\bold{T}}\hat{g}(y_i, y, |t-t_i|)}
\end{align}

\input{algorithms/Relative_conn}











\subsection{Explanation of \PNY}
\label{apdx:PNY}
Suppose that the variance and expectation of the representation from the previous layer are invariant with respect to the target node's time $t_1$. If we can specify $\mathcal{P}_{y_1 t_1}(y_2, t_2)$ for all cases, transformation of covariance matrix during the \PMP process could be calculated. \PNY numerically estimates the transformation of the covariance matrix during the \PMP process, and determines an affine transformation that can correct this variation.

The calculation of the $k+1$-th aggregated message $M_{i}^{pmp(k+1)}$ for the node $i$ described earlier is as follows:

\begin{align}
M_{i}^{pmp(k+1)} = {{2\sum_{v\in \mathcal{N}^{\text{single}}_i} \bold{x}_{v}^{(k)}+\sum_{v\in \mathcal{N}^{\text{double}}_i} \bold{x}_{v}^{(k)}}\over{2\big|\mathcal{N}^{\text{single}}_i\big| + \big|\mathcal{N}^{\text{double}}_i\big|}}
\end{align}

Here, $\mathcal{N}^{\text{single}}_i=\{v\in \mathcal{N}_i \big| t_v=t_i \text{ or }  t_v < 2t_i - t_{max} \}$, $\mathcal{N}^{\text{double}}_i=\{v\in \mathcal{N}_i\big| | t_v-t_i|\le| t_{max} - t_i | \}$. We previously proved that the expectation of $M_{i}^{pmp(k+1)}$ is time-invariant. Therefore, we can express $\mathbb{E}_{i\in \bold{V}_{yt}} [ M_i^{pmp (k+1)}] =\mu_{M}^{pmp(k+1)}(y_i)$, where $\bold{V}_{yt}= \{v\in \bold{V} \mid t_v = t, y_v = y\}$.

We will analyze how the covariance matrix of the aggregated message at node $i$ varies with time $t_i$, and label $y_i$, and define affine transformations to make them time-invariant.

\begin{align}
\text{var}(M_{i}^{pmp(k+1)})=\mathbb{E}\left[(M_i ^{pmp (k+1)}-\mu_{M}^{pmp(k+1)}(y_i))(M_i ^{pmp (k+1)}-\mu_{M}^{pmp(k+1)}(y_i))^{\top}\right]
\end{align}
We assume independence between representations from the previous layer. Suppose that the 2nd moment of representations from the previous layer is invariant. In other words, if $\text{var}(X_i^{(k)})=\text{var}(X_j^{(k)})\text{ s.t. }y_i=y_j$, then we can denote the 2nd moment as $\text{var}(X_i^{(k)})=\Sigma_{XX}^{pmp(k)}(y_i)$. Then 2nd moment of the aggregated message through \PMP is as follows: 
\begin{align}
\text{var}(M_{i}^{pmp(k+1)}) = {\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}4\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\right)\Sigma_{XX}^{pmp(k)}(y)
\over
\left(\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\right)^2}
\end{align}

This value depends not only on the label $y_i$ of the target node but also on $t_i$. Therefore, we can express $\text{var}(M_{i}^{pmp(k+1)})=\Sigma^{pmp(k+1)}_{MM}(y_i,t_i)$. Let's design an affine transformation to make it invariant over time. For a time $t$ where $t \neq t_{\text{max}}$ and for any $y$, generally $\Sigma^{pmp(k+1)}_{MM}(y_i,t_i)\neq\Sigma^{pmp(k+1)}_{MM}(y_i,t_{\text{max}})$.

Since the covariance matrix is always positive semi-definite, we can always orthogonally diagonalize it as $\Sigma^{pmp(k+1)}_{MM}(y,t)=P_tD_t P_t^{-1}$ and $\Sigma^{pmp(k+1)}_{MM}(y,t_{\text{max}})=P_{t_{\text{max}}}D_{t_{\text{max}}} P_{t_{\text{max}}}^{-1}$, where the diagonal elements of $D_{t}$ and $D_{t_{\text{max}}}$ are non-negative. Therefore, when $\text{var}(M_i^{pmp (k+1)})=\Sigma^{pmp(k+1)}_{MM}(y_i,t_i)$, $\mathbb E[M_i^{pmp(k+1)}]=\mu_M^{pmp (k+1)}({y_i})$, we can define the following affine transformation:

$M_{i}^{PNY(k+1)}\leftarrow A_{t_i} (M_i^{pmp(k+1)}-\mu_{M}^{pmp(k+1)}(y_i))+\mu_{M}^{pmp(k+1)}(y_i)$

At this point, it can be easily shown that $\mathbb{E}[M_{i}^{PNY(k+1)}]=\mu_{M}^{pmp(k+1)}(y)$ and $\text{var}(M_{i}^{PNY(k+1)})=A_{t_i}\Sigma^{pmp(k+1)}_{MM}(y_i,t_i)A{t_i}^{\top} = \Sigma^{pmp(k+1)}_{MM}(y_i,t_{max})$. In other words, if we can estimate $\Sigma^{pmp(k+1)}_{MM}(y,t)$ for any $y\in \bold{Y}, \ t\in \bold{T}$, then through affine transformation, we can make the 2nd moment of aggregated messages invariant over node time.


Based on the above estimations, we can formulate an estimator for ${\Sigma}_{MM}^{pmp(k+1)}(y_i, t_i)$ as follows.

\begin{align}
\hat{\Sigma}^{pmp(k+1)}_{MM}(y_i,t_i) = {\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}4\hat{\mathcal{P}}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\hat{\mathcal{P}}_{y_i t_i}(y, t)\right)\hat\Sigma_{XX}^{pmp(k)}(y)
\over
\left(\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\hat{\mathcal{P}}_{y_i t_i}(y, t)+\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\hat{\mathcal{P}}_{y_i t_i}(y, t)\right)^2}
\end{align}

Then, $\hat\Sigma^{pmp(k+1)}_{MM}(y_i,t_i)=\hat P_{y_i t_i}\hat D_{y_i t_i} \hat P_{y_i t_i}^{-1}$, $\hat\Sigma^{pmp(k+1)}_{MM}(y_i,t_{max})=\hat P_{y_i t_{max}}\hat D_{y_it_{max}} \hat P_{y_it_{max}}^{-1}$ can be orthogonally diagonalized.

Suppose we have all estimation $\hat{\mathcal P}_{y_i t_i}(y_j, t_j)$ for all $t_i, t_j \in \bold{T}$ and $y_i, y_j\in \bold{Y}$, as explained in \ref{apdx:rel_con}. Than, the \PNY transform can be expressed as follows.

\begin{align}
M_i^{PNY(k+1)}\leftarrow  \hat P_{y_i t_{max}}\hat D_{y_i t_{max}}^{1/2}\hat D_{y_i t_i}^{-1/2}\hat P_{y_i t_i}^{\top}(M_i^{pmp (k+1)}-\hat\mu_{M}^{pmp(k+1)}(y_i))+\hat \mu_{M}^{pmp(k+1)}(y_i)
\end{align}

As proven earlier, when the representation in the previous layer has 1st moment and 2nd moment invariant to the node's time, using \PMP and \PNY transform yields $\mathbb{E}[M_i^{PNY(k+1)}]=\mu_{M}^{pmp(k+1)}(y_i)$ and $\text{var}(M_i^{PNY(k+1)})=\Sigma^{pmp(k+1)}_{MM}(y_i,t_{max})$, ensuring that both the 1st order moment and 2nd order moment in the aggregated message become invariant to the node's time.


\input{algorithms/JJnorm}











\subsection{Explanation of \JJnorm}
\label{apdx:JJnorm}
As shown earlier, when passing through \PMP, the covariance matrix of the aggregated message is as follows.

\begin{align}
\Sigma^{pmp(k+1)}_{MM}(y_i,t_i) = {\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}4\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\right)\Sigma^{pmp(k)}_{XX}(y)
\over
\left(\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\right)\right)^2}
\end{align}

However, when $t_i=t_{max}$, $\bold{T}_{t_{max}}^{\text{double}}= \{t \in \bold{T}\ \big | \ |t- t_{max}|\le|t_{max}-t_{max}|, t\neq t_{max}\}=\phi$, making the covariance matrix simpler as follows.

\begin{align}
\Sigma^{pmp(k+1)}_{MM}(y_i,t_{max})= {\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}}4\mathcal{P}_{y_i t_i}(y, t)\Sigma_{XX}^{pmp(k)}(y)
\over
\left(\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}}2\mathcal{P}_{y_i t_i}(y, t)\right)^2}
\end{align}

To examine how the covariance matrix varies with time, let's consider the following two ratios.

\begin{align}
{\sum_{t\in\bold{T}_{t_i}^{\text{single}}}4\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\over \sum_{t\in\bold{T}}4\mathcal{P}_{y_i t_i}(y, t)}\\={4g(y_i, y, 0)+2\sum_{0<\tau\le |t_{max}-t_i|}g(y_i, y, \tau)+4\sum_{|t_{max}-t_i|<\tau}g(y_i, y, \tau)\over 4\sum_{0\le\tau}g(y_i,y,\tau)}=\gamma_{t_i}
\end{align}

\begin{align}
{\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\over \sum_{t\in\bold{T}}2\mathcal{P}_{y_i t_i}(y, t)}\\={2g(y_i, y, 0)+\sum_{0<\tau\le |t_{max}-t_i|}g(y_i, y, \tau)+2\sum_{|t_{max}-t_i|<\tau}g(y_i, y, \tau)\over 2\sum_{0\le\tau}g(y_i,y,\tau)}=\lambda_{t_i}
\end{align}

Here, we can denote these values as $\gamma_{t_i}$ and $\lambda_{t_i}$ because the value of $g(y_i, y, \tau)$ is invariant to $y_i$ and $y$ due to Assumption 4. Utilizing this, we can transform the equation as follows:

\begin{align}
\Sigma^{pmp(k+1)}_{MM}(y_i,t_i)= {\gamma_{t_i} \over \lambda_{t_i}^2}\Sigma^{pmp(k+1)}_{MM}(y_i,t_{max})
\end{align}

In other words, when Assumption 4 holds true, the covariance matrix of the aggregated message differs only by a constant factor, and this constant depends solely on the node's time. For simplicity, let's define $\alpha_{t} = {\lambda_{t}^2 \over \gamma_t}$, then we can express it as follows:

\begin{align}
\Sigma^{pmp(k+1)}_{MM}(y_i,t_{max})=\alpha_{t_i}\Sigma^{pmp(k+1)}_{MM}(y_i,t_i)
\end{align}

Unlike \PNY, which estimates an affine transformation using $\hat{\mathcal{P}}_{y_i t_i}(y_j, t_j)$ to align the covariance matrix to be invariant, \JJnorm provides a more direct method to obtain an estimate $\hat{\alpha}_{t_i}$ of $\alpha_{t_i}$.

Since we know that the covariance matrix differs only by a constant factor, we can simply use norms in multidimensional space rather than the covariance matrix to estimate $\alpha_{t_i}$.


Firstly, let's define $\bold{V}_{y,t} = \{u \in \bold{V} \mid y_u=y, t_u=t\}$, $\bold{V}_{\cdot,t} = \{u \in \bold{V} \mid t_u=t\}$. We can compute the mean of the aggregated message for each label and time: $\mu_M(\cdot,t) = \mathbb{E}_{i\in \bold{V}_{\cdot,t}}\left[M_i\right]$, $\mu_M(y,t) = \mathbb{E}_{i\in \bold{V}_{y,t}}\left[M_i\right]$. Here, \JJnorm is a process of transforming the aggregated message, which is aggregated through \PMP, into a time-invariant representation. Hence, we can suppose that $\mu_M(y,t)$ is invariant to $t$. That is, for all $t\in\bold{T}$, $\mu_M(y,t)=\mu_M(y,t_{max})$. Additionally, we can define the variance of distances as follows: $\sigma_{y,t}^2=\mathbb{E}_{i\in \bold{V}_{y,t}}\left[(M_i-\mu_M(y,t))^2\right]$, $\sigma_{\cdot,t}^2=\mathbb{E}_{i\in \bold{V}_{\cdot,t}}\left[(M_i-\mu_M(\cdot,t))^2\right]$. Here, the square operation denotes the L2-norm.

\begin{align}
\mathbb{E}_{i\in \bold{V}_{\cdot,t}}\left[(M_i-\mu_M(\cdot,t))^2\right] = \sum_{y\in \bold{Y}}P(y)\mathbb{E}_{i\in \bold{V}_{y,t}}\left[ (M_i - \mu_M(y,t)+\mu_M(y,t)-\mu_M(\cdot,t))^2\right]\\=\sum_{y\in \bold{Y}}P(y)\Big(\mathbb{E}_{i\in \bold{V}_{y,t}}\left[ (M_i - \mu_M(y,t))^2 \right] +(\mu_M(y,t)-\mu_M(\cdot,t))^2\Big)
\end{align}

Since $\mathbb{E}_{i\in \bold{V}_{y,t}}\left[ (M_i - \mu_M(y,t))^{\top}(\mu_M(y,t)-\mu_M(\cdot,t))\right]=0$.

Here, mean of the aggregated messages during training and testing times satisfies the following equation: $\mu_M(\cdot,t) = \mu_M(\cdot,t_{max})$

\begin{align}
\mu_M(\cdot,t)=\sum_{y\in\bold{Y}}P(y)\mu_M(y,t)=\sum_{y\in\bold{Y}}P(y)\mu_M(y,t_{max})=\mu_M(\cdot,t_{max})
\end{align}

This equation is derived from the assumption that $\mu_M(y,t)$ is invariant to $t$ and from Assumption 1 regarding $P(y)$. Furthermore, by using Assumption 1 again, we can show that the variance of the mean computed for each label is also invariant to $t$:

\begin{align}
\sum_{y\in\bold{Y}}P(y)\mathbb{E}_{i\in \bold{V}_{y,t}}\left[(\mu_M(y,t)-\mu_M(\cdot,t))^2 \right]=\sum_{y\in\bold{Y}}P(y)\mathbb{E}_{i\in \bold{V}_{y,t_{max}}}\left[(\mu_M(y,t_{max})-\mu_M(\cdot,t_{max}))^2 \right]
\end{align}

\begin{align}
\mathbb{E}_{i\in \bold{V}_{y,t}}\left[(\mu_M(y,t)-\mu_M(\cdot,t))^2 \right]=\mathbb{E}_{i\in \bold{V}_{y,t_{max}}}\left[(\mu_M(y,t_{max})-\mu_M(\cdot,t_{max}))^2\right] =\nu^2,\ t\in \bold{T}
\end{align}

Here, $\nu^2$ can be interpreted as the variance of the mean of messages from nodes with the same $t\in \bold{T}$ for each label. According to the above equality, this is a value invariant to $t$.

Meanwhile, from Assumption 4,

\begin{align}
\alpha_t \mathbb{E}_{i\in \bold{V}_{y,t}}\left[ (M - \mu_M(y,t))^2 \right] = \mathbb{E}_{i\in \bold{V}_{y,t_{max}}}\left[ (M - \mu_M(y,t_{max}))^2\right], \forall t\in \bold{T}
\end{align}

\begin{align}
\alpha_t\sum_{y\in\bold{Y}}P(y)\mathbb{E}_{i\in \bold{V}_{y,t}}\left[ (M_i - \mu_M(y,t))^2 \right]=\sum_{y\in\bold{Y}}P(y)\mathbb{E}_{i\in \bold{V}_{y,t_{max}}}\left[ (M_i - \mu_M(y,t_{max}))^2\right]
\end{align}

Adding $\nu^2$ to both sides,

\begin{align}
\alpha_t\sum_{y\in \bold{Y}}P(y)\mathbb{E}_{i\in \bold{V}_{y,t}}\left[ (M_i - \mu_M(y,t))^2 \right] +\sum_{y\in \bold{Y}}P(y)\mathbb{E}_{i\in \bold{V}_{y,t}}\left[(\mu_M(y,t)-\mu_M(\cdot,t))^2 \right] =\sigma_{\cdot,t_{max}}^2 
\end{align}

Thus,

\begin{align}
\alpha_t = { \sigma_{\cdot,t_{max}}^2  - \nu^2\over\sum_{y\in \bold{Y}}P(y)\mathbb{E}_{i\in \bold{V}_{y,t}}\left[ (M_i- \mu_M(y,t))^2 \right]}
\end{align}

Here, $\hat{\alpha}_t$ is an unbiased estimator of $\alpha_t$.

\begin{align}
\hat{\nu}^2={1\over \mid{\bold{V}_{\cdot,t}}\mid-1} \sum_{y\in \bold{Y}}\sum_{i \in \bold{V}_{y,t}}(\hat\mu_M(y,t) -\hat\mu_M(\cdot,t) )^2  
\end{align}


\begin{align}
\hat{\alpha}_t = {\left( {1\over \mid{\bold{V}_{\cdot,t_{max}}}\mid-1}\sum_{i\in \bold{V}_{\cdot,t_{max}}}(M_i-\hat\mu_M(\cdot,t_{max}))^2  -\hat{\nu}^2 \right)\over{1\over \mid{\bold{V}_{\cdot,t}}\mid-1} \sum_{y\in\bold{Y}} \sum_{i \in \bold{V}_{y,t}}(M_i-\hat\mu_M(y,t))^2}
\end{align}

Where $\hat\mu_M(y,t)={1\over {\mid \bold{V}_{y,t} \mid}}\sum_{i\in \bold{V}_{y,t}}M_i$  and $\hat\mu_M(\cdot,t) ={1\over {\mid \bold{V}_{\cdot,t} \mid}}\sum_{i\in \bold{V}_{\cdot,t}}M_i$ . 

Note that all three terms in the above equation can be directly computed without requiring test labels.

By using $\hat{\alpha_t}$, we can update the aggregated message from \PMP to align the second-order statistics.

\begin{align}
\ M_i^{JJnorm} \leftarrow (\hat\mu_M(y_i,t_i) -\hat\mu_M(\cdot,t_i) )+\hat{\alpha}_{t_i} (M_i - \hat\mu_M(y_i,t_i)),\ \forall i \in \bold{V}\setminus\bold{V}_{\cdot,t_{max}}
\end{align}

\input{algorithms/PNY}






\subsection{Lazy operation property of \JJnorm}
\label{apdx:Lazy}
Let's use mathematical induction. First, for initial features, $\Sigma_{XX}^{pmp (0)}(y,t_{max})= \Sigma_{XX}^{pmp (0)}(y,t)$ holds. Suppose that in the $k$-th layer, representation $X^{(k)}$ satisfies $\beta_{t}^{(k)}\Sigma_{XX}^{pmp (k)}(y,t_{max})= \Sigma_{XX}^{pmp (k)}(y,t)$. This assumes that the expected covariance matrix of representations of nodes with identical labels but differing time information only differs by a constant factor.

\begin{align}
\Sigma^{pmp(k+1)}_{MM}(y_i,t_i) = {\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}4\mathcal{P}_{y_i t_i}(y, t)\Sigma^{pmp(k)}_{XX}(y,t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\Sigma^{pmp(k)}_{XX}(y,t)\right)
\over
\left(\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\right)\right)^2}
\end{align}

\begin{align}
 = {\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}4\mathcal{P}_{y_i t_i}(y, t)\beta_{t}^{(k)}+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\beta_{t}^{(k)}\right)\Sigma_{XX}^{pmp (k)}(y,t_{max})
\over
\left(\sum_{y\in \bold{Y}}\left(\sum_{t\in\bold{T}_{t_i}^{\text{single}}}2\mathcal{P}_{y_i t_i}(y, t)+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\right)\right)^2}
\end{align}

By Assumption 4, following value is invariant to $y_i$.

\begin{align}
{\sum_{t\in\bold{T}_{t_i}^{\text{single}}}4\mathcal{P}_{y_i t_i}(y, t)\beta_t^{(k)}+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\beta_t^{(k)}\over \sum_{t\in\bold{T}}4\mathcal{P}_{y_i t_i}(y, t)\beta_t^{(k)}}=\gamma_{t_i}^{(k)}
\end{align}

Furthermore, using the previously defined $\lambda_{t_i}$,

\begin{align}
\Sigma^{pmp(k+1)}_{MM}(y_i,t_i) = {\gamma_{t_i}^{(k)}\over\lambda_{t_i}^2} {\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}}4\mathcal{P}_{y_i t_i}(y, t)\beta_t^{(k)}\Sigma_{XX}^{pmp (k)}(y,t_{max})
\over
\left(\sum_{y\in \bold{Y}}\sum_{t\in\bold{T}}2\mathcal{P}_{y_i t_i}(y, t)\right)^2} = {\gamma_{t_i}^{(k)}\over\lambda_{t_i}^2} \Sigma^{pmp(k+1)}_{MM}(y_i,t_{max}) 
\end{align}

Since $X_i^{(k+1)}=A^{(k+1)}M_i^{(k+1)}$, the following equation holds.

\begin{align}
\Sigma^{pmp(k+1)}_{XX}(y_i,t_i)= A^{(k+1)}\Sigma^{pmp(k+1)}_{MM}(y_i,t_i)A^{(k+1)\top}=\\A^{(k+1)}{\gamma_{t_i}^{(k)}\over\lambda_{t_i}^2}\Sigma^{pmp(k+1)}_{MM}(y_i,t_{max})A^{(k+1)\top} ={\gamma_{t_i}^{(k)}\over\lambda_{t_i}^2} \Sigma^{pmp(k+1)}_{MM}(y_i,t_{max}) 
\end{align}

Here, $\beta_t^{(k+1)}$ is recursively defined as follows.

\begin{align}
\beta_t^{(k+1)} = {\gamma_{t_i}^{(k)}\over\lambda_{t_i}^2}={\sum_{t\in\bold{T}_{t_i}^{\text{single}}}4\mathcal{P}_{y_i t_i}(y, t)\beta_t^{(k)}+\sum_{t\in\bold{T}_{t_i}^{\text{double}}}\mathcal{P}_{y_i t_i}(y, t)\beta_t^{(k)}\over \lambda_{t_i}^2\sum_{t\in\bold{T}}4\mathcal{P}_{y_i t_i}(y, t)\beta_t^{(k)}}
\end{align}

Therefore, it is proven that $\beta_{t}^{(k)}\Sigma_{XX}^{pmp (k)}(y,t_{max})= \Sigma_{XX}^{pmp (k)}(y,t)$ holds for all representations for any $k\le K$.



\subsection{Scalability of invariant message passing methods}
\label{apdx:scalability}
First moment alignment methods such as \MMP and \PMP have the same complexity and can be easily applied by modifying the graph. By adding or removing edges according to the conditions, only $\mathcal{O}(|E|)$ additional preprocessing time is required, which is necessary only once throughout the entire training process. If the graph cannot be modified and the message passing function needs to be modified instead, it would require $\mathcal{O}(|E|fK)$, which is equivalent to the traditional averaging message passing. Similarly, the memory complexity remains $\mathcal{O}(|E|fK)$, consistent with traditional averaging message passing. Despite having the same complexity, \PMP is much more expressive than \MMP. Unless there are specific circumstances, \PMP is recommended for first moment alignment.

In \PNY, estimating the relative connectivity $\hat{\mathcal{P}}_{y_i, t_i}(y_j, t_j)$ requires careful consideration. If both $t_i\neq t_{\text{max}}$ and $t_j\neq t_{\text{max}}$, calculating the relative connectivity for all pairs involves $\mathcal{O}((N+|E|)f)$ operations, while computing for cases where either time is $t_{\text{max}}$ requires $\mathcal{O}(|Y|^2|T|^2)$ computations. Therefore, the total time complexity becomes $\mathcal{O}(|Y|^2|T|^2+(N+|E|)f)$. Additionally, for each message passing step, the covariance matrix of the previous layer's representation and the aggregated message needs to be computed for each label-time pair. Calculating the covariance matrix of the representation from the previous layer requires $\mathcal{O}((|Y||T|+N)f^2)$ operations. Subsequently, computing the covariance matrix of the aggregated message obtained through \PMP via relative connectivity requires $\mathcal{O}(|Y|^2|T|^2f^2)$ operations. Diagonalizing each of them to create affine transforms requires $\mathcal{O}(|Y||T|f^3)$, and transforming all representations requires $\mathcal{O}(Nf^2)$. Thus, with a total of $K$ layers of topological aggregation, the time complexity for applying \PNY becomes $\mathcal{O}(K(|Y||T|f^3+|Y|^2|T|^2f^2+Nf^2)+|E|f)$. Additionally, the memory complexity includes storing covariance matrices based on relative connectivity and label-time information, which is $\mathcal{O}(|Y||T|f^2 + |Y|^2|T|^2)$.

Now, let's consider applying \PNY to real-world massive graph data. For instance, in the ogbn-mag dataset, $|Y|=349$, $|T|=11$, $N=629571$, and $|E|=21111007$. Assuming a representation dimension of $f=512$, it becomes apparent that performing at least several trillion floating-point operations is necessary. Without approximation or transformations, applying \PNY to large graphs becomes challenging in terms of scalability.

Lastly, for \JJnorm, computing the sample mean of aggregated messages for each label and time pair requires $\mathcal{O}(Nf)$ operations. Based on this, computing the total variance, variance of the mean, and mean of representations with each time requires $\mathcal{O}(Nf)$ operations. Calculating each $\hat\alpha_t$ requires $O(|T|)$ operations, and modifying the aggregated message based on this requires $\mathcal{O}(Nf)$ operations, resulting in a total of $\mathcal{O}(Nf+|T|) \simeq \mathcal{O}(Nf)$ operations. With a total of $K$ layers, this requires $\mathcal{O}(NfK)$ operations, but GNNs with linear nodewise semantic aggregation functions have the property of lazy operation, reducing the time complexity to $\mathcal{O}(Nf)$. Additionally, the memory complexity becomes $\mathcal{O}(|Y||T|f)$. Considering that most operations in \JJnorm can be parallelized, it exhibits excellent scalability.

In experiments with synthetic graphs, it was shown that invariant message passing methods can be applied to general spatial GNNs, not just decoupled GNNs. For 1st moment alignment methods such as \PMP and \MMP, which can be applied by reconstructing the graph, they have the same time and memory complexity as calculated above. However, for 2nd moment alignment methods such as \JJnorm or \PNY, transformation is required for each message passing step, resulting in a time complexity multiplied by the number of epochs as calculated above. Therefore, when using general spatial GNNs on real-world graphs, only 1st moment alignment methods may be realistically applicable.

\textbf{Guidelines for deciding which \IMPaCT method to use.} Based on these findings, we propose guidelines for deciding which invariant message passing method to use. If the graph exhibits differences in environments due to temporal information, we recommend starting with \PMP to make the representation's 1st moment invariant during training. \MMP is generally not recommended. Next, if using Decoupled GNNs, \PNY and \JJnorm should be compared. If the graph is too large to apply \PNY, compare the results of using \PMP alone with using both \PMP and \JJnorm. In cases where there are no nonlinear operations in the message passing stage, \JJnorm needs to be applied only once at the end. Using 2nd moment alignment methods with General Spatial GNNs may be challenging unless scalability is improved.

Caution is warranted when applying invariant message passing methods to real-world data. If Assumptions do not hold or if the semantic aggregation functions between layers exhibit loose Lipschitz continuity, the differences in the distribution of final representations over time cannot be ignored. Therefore, rather than relying on a single method, exploring various combinations of the proposed invariant message passing methods to find the best-performing approach is recommended.
