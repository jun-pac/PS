\label{sec:firstorder}
Message passing refers to the process of aggregating representations from neighboring nodes in the previous layer. Here, we assume the commonly used averaging message passing procedure. For any arbitrary target node $v\in\bold{V}$ with label $y$ and time $t$,

\vspace{-15pt}
\begin{align}
    M_{ v}^{(k+1)}={\sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\in\bold{T}}\sum_{w\in\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right)}X_w \over \sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\in\bold{T}} |\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right)|} , X_w \overset{\text{IID}}{\sim} {x_{\tilde{y}\tilde{t}}^{(k)}} \; \text{for} \; \forall w\in\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right)
\end{align}
\vspace{-5pt}

\begin{wrapfigure}{R}{0.2\textwidth}
    \vspace{-15pt}
    \centering
    \includegraphics[width=0.2\textwidth]{figs/PMP.png}
    \caption{Graphical explanation of \PMP}
    \label{fig:PMP}
    \vspace{-30pt}
\end{wrapfigure}

where $\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right) = \{w \in \bold{V} \mid w \; \text{is a neighbor of} \; v \; \text{which has label} \; \tilde{y} \; \text{and time} \; \tilde{t} \}$. 
$M_v^{(k+1)}$ is the aggregated message at node $v$ in the $k+1$-th layer, and $x_{yt}^{(k)}$ is the distribution of representations from the previous layer. For simplification, the term "$X_w \overset{\text{IID}}{\sim} {x_{\tilde{y}\tilde{t}}^{(k)}} \; \text{for} \; \forall w\in\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right)$" will be omitted in definitions in the subsequent discussions.\\ %, and we will consider $v$ as the target node with label $y$ and $t$ if there are no other specifications. \\
\textit{\textbf{The first moment of aggregated message.}} If the representations from the previous layer have means which are consistent across time, i.e., ${\mathbb{E}}_{X\sim {x_{\tilde{y}\tilde{t}}^{(k)}}}\left[X\right]=\mu_{X}^{(k)}(\tilde{y})$, we can calculate the approximate expectation, defined in Appendix \ref{apdx:firstmm}, as $\hat{\mathbb E}[{M_{ v}^{(k+1)}}]=\sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\in\bold{T}}\mathcal{P}_{y t}\left(\tilde{y}, \tilde{t}\right)\mu_{X}^{(k)}(\tilde{y})$.

% \vspace{-15pt}
% \begin{align}
%     \hat{\mathbb E}[{M_{ v}^{(k+1)}}]=\sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\in\bold{T}}\mathcal{P}_{y t}\left(\tilde{y}, \tilde{t}\right)\mu_{X}^{(k)}(\tilde{y})
% \end{align}
% \vspace{-5pt}

% and let us call it \textit{the first moment of aggregated message}. See Appendix \ref{apdx:firstmm} for the details of subtle difference between the true expectation $\mathbb{E}$ and the approximate expectation $\hat{\mathbb{E}}$. \\
% We can observe that in contrast to ${\mathbb{E}}_{X\sim {x_{\tilde{y}\tilde{t}}^{(k)}}}\left[X\right]$, $\hat{\mathbb E}[{M_{ v}^{(k+1)}}]$ remains dependent on the node's time $t$ due to  $\mathcal{P}_{y t}\left(\tilde{y}, \tilde{t}\right)$. Our objective is to modify the spatial aggregation method to ensure that the final representation is obtained by collecting all invariant topological information. 
We can observe that $\hat{\mathbb E}[{M_{ v}^{(k+1)}}]$ depends on node's time $t$ due to  $\mathcal{P}_{y t}\left(\tilde{y}, \tilde{t}\right)$. Our objective is to modify the spatial aggregation method to ensure invariance of 1st moment is preserved.
%Here, we propose an improved message passing method to ensure that the 1st moment of the aggregated message obtained through message passing is invariant with respect to time.

\subsection{Persistent Message Passing: \PMP}
We propose Persistent Message Passing (\PMP) as one approach to achieve 1st moment alignment,.

For the target node $v$ with time $t$, consider the time $\tilde t$ of some neighboring node. For $\Delta=  | \tilde t - t|$ where $0<\Delta \le | t_{max}-t|$, both $t + \Delta$ and $t - \Delta$ neighbor nodes can exist. However, nodes with $\Delta >|t_{max}-t|$ or $\Delta = 0$ are only possible when $\tilde t=t-\Delta$. Let $\bold{T}_{t}^{\text{double}} = \{\tilde t \in \bold{T} \mid 0<|\tilde t - t| \le | t_{max}-t|\}$ and $\bold{T}_{t}^{\text{single}} = \{\tilde t \in \bold{T} \mid | \tilde t - t| > | t_{max}-t| \; \text{or} \; \tilde t = t\}$. As discussed, the target node receives twice the weight from $\tilde t \in \bold{T}_{t}^{\text{double}}$ against $\tilde t \in \bold{T}_{t}^{\text{single}}$. Motivation behind \PMP is to correct this by double weighting the neighbor nodes with time in $\bold{T}_{t}^{\text{single}}$, as depicted in Figure \ref{fig:PMP}.


% \vspace{-15pt}
% \begin{align}
%     \bold{T}_{t_i}^{\text{double}} = \{t_j \in \bold{T} \mid 0<| t_j - t_i| \le | t_{max}-t_i|\}\\
%     \bold{T}_{t_i}^{\text{single}} = \{t_j \in \bold{T} \mid | t_j - t_i| > | t_{max}-t_i| \text{or} t_j = t_i\}
% \end{align}
% \vspace{-15pt}




\begin{definition}\label{def:env1}
The \PMP from the $k$-th layer to the $k+1$-th layer of target node $v$ is defined as:

\vspace{-15pt}
\begin{align}\label{eqn:pmp}
M_{ v}^{pmp(k+1)} &= {\sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\in\bold{T}_{t}^{\text{single}}}\sum_{w\in\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right)}2X_w+\sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\in\bold{T}_{t}^{\text{double}}}\sum_{w\in\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right)}X_w \over \sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\in\bold{T}_{t}^{\text{single}}}2|\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right)|+\sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\in\bold{T}_{t}^{\text{double}}}|\mathcal{N}_{v}\left(\tilde{y}, \tilde{t}\right)|}
\end{align}
\vspace{-5pt}
\end{definition}

\begin{theorem}\label{thm:pmp}
The 1st moment of aggregated message obtained by \PMP layer is invariant, if the 1st moment of previous representation is invariant. \\
\textbf{Sketch of proof} Let $\mathbb{E}_{X\sim{x_{\tilde{y} \tilde{t}}^{(k)}}}\left[X\right]=\mu_{X}^{(k)}(\tilde{y})$ as a function invariant with $\tilde{t}$. Then we can get

\vspace{-15pt}
\begin{align}
\hat{\mathbb E}\left[{M_{ v}^{pmp(k+1)}}\right]={\sum_{\tilde{y}\in \bold{Y}}\sum_{\tau\ge 0}g(y, \tilde{y}, \tau)\mu_{X}^{(k)}(\tilde{y})\over\sum_{\tilde y\in \bold{Y}}\sum_{\tau\ge 0}g(y, \tilde{y}, \tau)}
\end{align}
\vspace{-5pt}
which is also invariant with $t$. See Appendix \ref{apdx:PMP} for details and implementation.
\end{theorem}



\subsection{Mono-directional Message Passing: \MMP}
\PMP is not the only method for achieving 1st order alignment. There can be infinitely many ways to adjust the distribution of absolute relative times to be consistent regardless of the target node's time. We introduce Mono-directional Message Passing (\MMP) as one such approach. \MMP aggregates information only from neighboring nodes whose times are the same as or earlier than the target node.

\begin{definition}\label{def:env3}
The \MMP from the $k$-th layer to the $k+1$-th layer of target node $v$ is defined as:

\vspace{-15pt}
\begin{align}
    M_{v}^{mmp(k+1)} = {\sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\le t}\sum_{w\in\mathcal{N}_{v}(\tilde y, \tilde t)}X_w \over \sum_{\tilde{y}\in \bold{Y}}\sum_{\tilde{t}\le t}|\mathcal{N}_{v}(\tilde y, \tilde t)|}
\end{align}
\vspace{-5pt}
\end{definition}

\begin{theorem}\label{thm:mmp}
The 1st moment of aggregated message obtained by \MMP layer is invariant, if the 1st moment of previous representation is invariant.\\
\textbf{Sketch of proof} Let $\mathbb{E}_{X\sim{x_{\tilde{y} \tilde{t}}^{(k)}}}\left[X\right]=\mu_{X}^{(k)}(\tilde{y})$ as a function invariant with $\tilde{t}$. Then we can calculate

\vspace{-15pt}
\begin{align}
\hat{\mathbb E}\left[{M_{ v}^{mmp(k+1)}}\right]={\sum_{\tilde{y}\in \bold{Y}}\sum_{\tau\ge 0}g(y, \tilde{y}, \tau)\mu_{X}^{(k)}(\tilde{y})\over\sum_{y\in \bold{Y}}\sum_{\tau\ge 0}g(y, \tilde{y}, \tau)}
\end{align}
\vspace{-5pt}
which is also invariant with $t$. See Appendix \ref{apdx:MMP} for details and implementation.
\end{theorem}

\textbf{Comparison between \PMP and \MMP.} Both \PMP and \MMP adjust the weights of messages collected from neighboring nodes that meet certain conditions, either doubling or ignoring their impact. They can be implemented easily by reconstructing the graph according to the conditions without any modifications to the existing code. However, \MMP collects less information since it only gathers information only from the past during message passing, resulting a smaller ego-graph.
% the effective ego-graph size that affects the final representation decreases exponentially with the number of layers. A decrease in the amount of collected information leads to an increase in the variance of the final representation, resulting in reduced prediction accuracy.

It is important to note that not all invariant information is meaningful. Our ultimate goal is to obtain a maximally informative invariant representation \cite{arjovsky2019invariant}. Since \MMP arbitrarily reduces the effective ego-graph, it cannot be said to meet the requirements for being maximally informative. Therefore, \PMP will be used as the 1st moment alignment method in the subsequent discussions. Furthermore, from Theorem \ref{thm:pmp}, we will denote $\hat{\mathbb{E}}\left[{M_{v}^{pmp(k+1)}}\right]$ as $\mu_{M}^{pmp(k+1)}(y)$ for target node $v$ with label $y$.

% It is important here that not all invariant information is our interest. For instance, if the final representation is constant, it is invariant, but it does not provide meaningful information for classification. In other words, we are interested in invariant representation that is maximally informative \cite{arjovsky2019invariant}. Thus, since \MMP is arbitrarily reducing the effective ego-graph, so it cannot be said that it satisfies the requirements for maximally informative. Based on these analysis, \PMP will be used as the 1st order alignment method in the subsequent discussions. Furthermore, according to Theorem \ref{thm:pmp}, from now on we will write $\hat{\mathbb E}\left[{M_{ v}^{pmp(k+1)}}\right]$ as $\mu_{M}^{pmp(k+1)}(y)$ for target node $v$ with label $y$.

% To summarize, \PMP not only performs better experimentally compared to \MMP as an invariant message passing method but is also the most straightforward and intuitive method to satisfy the necessary condition for maximizing informativeness. \PMP has almost no additional overhead compared to traditional averaging message passing and can be easily applied in practice by simply duplicating edges belonging to $\mathcal{N}_i^{\text{single}}$, that is, by reconstructing the graph. Moreover, it is more expressive than \MMP due to its larger effective ego-graph size. Given these advantages, \PMP will be used as the 1st order alignment method in the subsequent discussions.

\subsection{Theoretical analysis of \PMP when applied in multi-layer GNNs.}\label{sec:analysis}
We will assume the messages and representation to be scalar in this discussion. Let $\mathcal{M}^{(k)}$ as the space of messages at $k$-th layer, and $\mathcal{X}^{(k)}$ as the space of representations at $k$-th layer. We modeled \PMP with probability measures as in Appendix \ref{apdx:pmp_modeling}. Now suppose that, (i) $|M_v^{(k)}|\le C$ almost surely for $\forall v\in V$, $M_v^{(k)}\sim m_{yt}^{(k)}$, and (ii) $\text{var}(M_v^{(k)})\le V$ for $\forall v\in \bold{V}$. Since we are considering 1st moment alignment method, \PMP, we may assume $\mathbb{E}[M_v^{(k)}]=\hat{\mathbb{E}}[M_v^{(k)}]=\mu_{M}^{(k)}(y)$ for $M_v^{(k)}\sim m_{yt}^{(k)},\ \forall y\in \bold{Y}, \forall t\in \bold{T}$. Here, $W_1$ is the Wasserstein-1 metric of probability measures, and we assume G-Lipschitz condition for semantic aggregation functions, $f^{(k)},\ \forall k\in\{1,2,\dots,K\}$.

\begin{theorem}\label{thm:thm1}
    $W_1 (m_{yt}^{(k)},m_{yt¡¯}^{(k)})\le \mathcal{O}(C^{1/3}V^{1/3})$
\end{theorem}
\begin{theorem}\label{thm:thm2}
    If $m_{yt}^{(k)}$ is sub-Gaussian with constant $\tau$, then $W_1(m_{yt}^{(k)}, m_{yt'}^{(k)})\le \mathcal{O}(\tau\sqrt{\log C})$.
\end{theorem}
\begin{theorem}\label{thm:thm3}
    If $\forall y\in \bold{Y}, \forall t, t'\in \bold{T}$, $W_1(m_{yt}^{(k)},m_{yt'}^{(k)})\le W,$ then for $\forall y\in \bold{Y}, \forall t, t'\in \bold{T}$, $W_1(m_{yt}^{(k+1)},m_{yt'}^{(k+1)})\le {G\over G^{(k)}}W$ where $G^{(k)}>1$ is a constant only depending on the layer $k$.
\end{theorem}
\begin{corollary}\label{thm:cor3}
    $W_1(m_{yt}^{(k)},m_{yt_{max}}^{(k)})\le {G^{K-1}\over {G^{(1)}G^{(2)}\dots G^{(K-1)}}}\mathcal{O}(\min\{C^{1/3}V^{1/3},\tau\sqrt{\log C}\})$
\end{corollary}
All proofs are provided in Appendix \ref{apdx:pmp_theory}. Here we ensured that the $W_1$ distance between train and test distributions of final representations are bounded when \PMP is applied in multi-layer GNNs. This is a meaningful theoretical analysis since Wasserstein distances play a  significant role on domain adaptation bounds, both theoretically and practically \cite{shen2018wasserstein,arjovsky2017wasserstein}.