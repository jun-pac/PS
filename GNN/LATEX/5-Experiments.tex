\subsection{Synthetic chronological split dataset}
\textbf{Temporal Stochastic Block Model(TSBM).} To assess the robustness and generalizability of proposed invariant message passing methods on graphs satisfying assumptions 1, 2, and 3, we conducted experiments on synthetic graphs. In order to create repeatable and realistic chronological graphs, we defined the Temporal Stochastic Block Model(TSBM) as our graph generation algorithm. TSBM can be regarded as a special case of the Stochastic Block Model(SBM) that incorporates temporal information of nodes \cite{holland1983stochastic,deshpande2018contextual}. In the SBM, the probability matrix $\bold{P}_{y \tilde y}$ represents the probability of a connection between two nodes $i$ and $j$, where $y$ and $\tilde y$ denote the communities to which the nodes belong. Our study extends this concept to account for temporal information, differentiating communities based on both node labels and time. In the Temporal Stochastic Block Model (TSBM), the connection probability is represented by a 4-dimensional tensor $\bold{P}_{t \tilde t y \tilde y}$. We ensured that Assumptions 1, 2, and 3 were satisfied. Specifically, the feature assigned to each node $\bold{x} \in \mathbb{R}^f$ was sampled from distributions depending solely on the label, defined as $\bold{x}_i = \mu(y) + k_{y} Z_i$. Here, $Z_i \in \mathbb{R}^f$ is an IID standard normal noise and $k_{y}$ represents the variance of features. To satisfy Assumption 2, the time and label of each node were determined independently. To satisfy Assumption 3, we first considered the possible forms of $g(y, \tilde y, |t - \tilde t|)$ and then determined $\bold{P}_{t \tilde t y \tilde y}$ accordingly. We used an exponentially decaying function with decay factor $\gamma_{y, \tilde y}$, defined as:

\vspace{-15pt}
\begin{align}
    g(y, \tilde y, |t - \tilde t|) = \gamma_{y, \tilde y}^{|t - \tilde t|} g(y , \tilde y, 0),\ \forall |t - \tilde t|>0
\end{align}
\vspace{-5pt}

\vspace{-10pt}
\textbf{Experimental Setup.} For our experiments, we employed the simplest and most fundamental decoupled GNN, Simple Graph Convolution (SGC) \cite{SGC}, as the baseline model. Additionally, we investigated whether the methods proposed in this study could improve the performance of general spatial GNNs. Hence, we used a 2-layer Graph Convolutional Network (GCN) \cite{GCN} that performs averaging message passing as another baseline model. We applied the \MMP, \PMP, \PMP+\PNY, and \PMP+\JJnorm methods to the baselines and compared their performance. Here, since the semantic aggregation of GCN is nonlinear, layer-wise \JJnorm was applied, i.e. \JJnorm could not be applied only to the aggregated message in the last layer but was applied to the aggregated message in each layer.  To test the generalizability of \JJnorm, which is based on Assumption 4, we conducted experiments on graphs that both satisfy and do not satisfy this assumption. Furthermore, for cases where Assumption 4 was satisfied, common decay factor $\gamma$ can be defined. A smaller $\gamma$ corresponds to a graph where the connection probability decreases drastically. We also compared the trends in the performance of each \IMPaCT method by varying the value of $\gamma$. Detailed settings are provided in Appendix \ref{apdx:synthetic_setup}. The results on synthetic graphs are presented in Table \ref{table:synthetic} and Figure \ref{fig:synthetic}.% This essentially interprets a multilayer GCN as a cascade of single-layer GCNs.


% For cases where Assumption 4 was not satisfied, $\gamma_{y_i, y_j}$ was sampled from a uniform distribution $[0.4, 0.7]$. For cases where Assumption 4 was satisfied, all decay factors were the same, i.e., $\gamma_{y_i, y_j} = \gamma, \ \forall y_i, y_j \in \bold{Y}$. In this case, $\gamma$ indicates the extent to which the connection probability varies with the time difference between two nodes. A smaller $\gamma$ corresponds to a graph where the connection probability decreases drastically. We also compared the trends in the performance of each \IMPaCT method by varying the value of $\gamma$. The baseline SGC consisted of 2 layers of message passing and 2 layers of MLP, with the hidden layer dimension set to 16. The baseline GCN also consisted of 2 layers with the hidden layer dimension set to 16. Adam optimizer was used for training with a learning rate of $0.01$ and a weight decay of $0.0005$. Each model was trained for 200 epochs, and each data was obtained by repeating experiments on 200 random graph datasets generated through TSBM. For the hyperparameters $\mathcal{K}$ and $\mathcal{G}$, we used $\mathcal{K} = 0.6$ and $\mathcal{G} = 0.24$. The training of both models was conducted on a 2x Intel Xeon Platinum 8268 CPU with 48 cores and 192GB RAM. 
\begin{figure}[!hbt]
    \vspace{-5pt}
	\centering
	\includegraphics[width=\textwidth]{figs/synthetic_hor.png}
	\caption{ The left graphs show the performance gain of \IMPaCT over the baseline. The right graphs illustrate the gain of 2nd moment alignment methods over the 1st moment alignment method \PMP.}
	 \label{fig:synthetic}
  \vspace{-10pt}
\end{figure}


\vspace{3pt}
\subsection{Real world chronological split dataset}

To assess the performance of invariant message passing on real-world datasets, we utilized the ogbn-mag dataset from the OGB \cite{OGB}. The statistics of ogbn-mag dataset is summarized in Appendix \ref{apdx:toy_experiment}. We employed CLGNN \cite{CLGNN} as the baseline model, which is based on RpHGNN \cite{RpHGNN} and incorporates a curriculum learning approach. RpHGNN, a decoupled GNN, effectively resolves the trade-off between the amount of information and computational overhead in message passing by random projection squashing technique. Since overall semantic aggregation processes in RpHGNN are linear, \JJnorm can be applied. However, due to the immense size of the graph, the application of \PNY was challenging. Therefore, we applied the \MMP, \PMP, and \PMP+\JJnorm to baseline and compared the performance. Each experiment was repeated 9 times, and the hyperparameters were set same as those in Wong et al. \cite{CLGNN}. Entire training process took 5h 42m, on a Tesla P100 GPU machine with 28 Intel Xeon 2680 CPUs and 128GB of RAM. Results are shown in Table \ref{table:ogbn-mag}. 

%The ogbn-mag dataset features a chronological split and consists of a heterogeneous graph. The task for ogbn-mag graph involves classifying the venue of papers (conference or journal) into 349 different classes. The graph comprises four types of nodes, including papers, authors, institutions, and fields of study, along with four distinct types of edges. 
% In heterogeneous graphs, not all nodes may possess temporal information. We can only apply invariant message passing methods when message passing occurs between nodes of types with temporal information.

% In the case of \MMP, learning was unstable in some cases. Therefore, both results excluding cases of failure in convergence and statistics including cases of failure in convergence were presented. 
% Preprocessing was performed on a 48 core Intel Xeon Platinum 8268 CPU with 768GB of RAM. Training took place on a Tesla P100 GPU with 28 Intel Xeon E5-2680 CPUs and 128GB of RAM. 




% The time complexity of \IMPaCT methods is as shown in Table \ref{table:scalability}. For detailed analyses, refer to Appendix \ref{apdx:scalability}. All methods exhibit linear complexity with respect to the number of nodes and edges. When employed in Decoupled GNNs, the operations of \IMPaCT methods occur only during preprocessing. When applied to general spatial GNNs, however, for \PNY and \JJnorm, the scalability may vary depends on the $f$, $|\bold{Y}|$, $|\bold{T}|$, and type of baseline model. 
% \JJnorm, when used in conjunction with \PMP in most decoupled GNNs, with very high scalability and adaptability. As it can obtain invariant information without modifying the message passing function of the baseline model, it enables effective operation with minimal cost when applied to chronological split datasets.

% In particular, \PMP demonstrated superior performance in experiments using both real and synthetic graphs, surpassing the baseline by a significant margin. It offers a method applicable with a single operation for reconstructing the graph, regardless of the model, if the graph can be reconstructed. Therefore, it exhibits good adaptability and scalability, robustly addressing domain adaptation induced by chronological split. Thus, if the graph exhibits differences in environments due to temporal information, we recommend starting with \PMP to make the representation's 1st moment invariant during training.

% \PMP demonstrated superior performance in experiments using both real and synthetic graphs, and it offers a method applicable with a single operation for reconstructing the graph. Other guidelines for selecting \IMPaCT models in chronological split dataset are provided in Appendix \ref{apdx:scalability}.


\subsection{Results}

\input{TABLE/ogbn-mag}
\input{TABLE/synthetic}

\textbf{Experimental results.} Our experiments using both real and synthetic graphs demonstrated the superior performance of \IMPaCT methods. By employing \PMP+\JJnorm, we achieved a significant performance improvement of 6.1\% over the current state-of-the-art methods on the ogbn-mag dataset. In 1st moment alignment, \PMP consistently outperformed the baseline model significantly, irrespective of the settings. Specifically, on the ogbn-mag dataset, applying \PMP resulted in a 5.1\% increase in accuracy compared to the baseline. In the experiments with synthetic graphs, \PMP provided a performance gain of 4.7\% with SGC and 2.4\% with GCN over their baselines. 2nd moment alignment methods generally performed better than 1st moment alignment methods alone. In ogbn-mag, \JJnorm outperformed \PMP by 1.0\%. In synthetic datasets, \JJnorm almost always performed better than \PNY, except in cases where Assumption 4 did not hold and the baseline model was GCN. Experimental results further support the generalizability of our methods. Even when using general spatial GNNs as the baseline model, \IMPaCT provided significant performance gains. Additionally, \JJnorm improved performance over \PMP even when Assumption 4 did not hold. %We present guidelines for selecting \IMPaCT models in chronological split datasets in Appendix \ref{apdx:scalability}.

\input{TABLE/scalability}
\textbf{Scalability.} The time complexity of \IMPaCT methods is shown in Table \ref{table:scalability}. For detailed analyses, refer to Appendix \ref{apdx:scalability}. All methods exhibit linear complexity with respect to the number of nodes and edges. When employed in Decoupled GNNs, the operations of \IMPaCT methods occur only during preprocessing. Specifically, \PMP and \JJnorm can be implemented with only graph modification and final representation correction, thus offering not only very high scalability but also adaptability. However, when applied to general spatial GNNs, the operations of \IMPaCT are multiplied by the number of epochs, making it challenging to consistently maintain scalability for \PNY and \JJnorm. Nevertheless, since most operations of \IMPaCT can be parallelized, the actual training time can vary significantly depending on the implementation.
